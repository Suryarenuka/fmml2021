{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module3_Lab3_FMML2021620.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyML8VU3MzGt3SAVE3VoMwd0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suryarenuka/fmml2021/blob/main/Module3_Lab3_FMML2021620.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FOUNDATIONS OF MODERN MACHINE LEARNING, IIIT Hyderabad**\n",
        "\n",
        "MODULE: CLASSIFICATION-1\n",
        "\n",
        "LAB-3 : Using KNN for Text Classification\n",
        "\n",
        "Module Coordinator: Sahil Manoj Bhatt\n",
        "\n",
        "NOTE: YOU ONLY NEED TO MAKE CHANGES/WRITE CODE IN CELLS THAT SPECIFICALLY MENTION TASK-1, TASK-2, etc.\n",
        "\n",
        "WRITE ANY OBSERVATION(S), IF REQUIRED BY THE TASK, IN A SEPARATE CELL AT THE BOTTOM OF THE NOTEBOOK."
      ],
      "metadata": {
        "id": "Y1U2FSvAzBjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK (or Natural Language Tool Kit) is a commonly used library for processing text.\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "ErytnDquzVYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning and Preprocessing step**"
      ],
      "metadata": {
        "id": "o_26e_B9zZ81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BAG OF WORDS\n",
        "\n",
        "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n",
        "\n",
        "The approach is very simple and flexible, and can be used in many ways for extracting features from documents.\n",
        "\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document. It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document."
      ],
      "metadata": {
        "id": "OW8eoQQ4zm9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to convert document(s) to a list of words, with the option of removing stopwords. Returns document-term matrix.\n",
        "\n",
        "def createBagOfWords(train, test, remove_stopwords, lemmatize, stemmer):\n",
        "    if remove_stopwords:\n",
        "        vectorizer = CountVectorizer(analyzer='word', input='content', stop_words=stopwords.words('english'))\n",
        "    else:\n",
        "        vectorizer = CountVectorizer(analyzer='word', input='content')\n",
        "\n",
        "    clean_train = []\n",
        "    for paragraph in train:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_train.append(paragraph)\n",
        "\n",
        "    clean_test = []\n",
        "    for paragraph in test:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_test.append(paragraph)\n",
        "\n",
        "    bag_of_words_train = vectorizer.fit_transform(clean_train).toarray()\n",
        "    bag_of_words_test = vectorizer.transform(clean_test).toarray()\n",
        "    return bag_of_words_train, bag_of_words_test\n"
      ],
      "metadata": {
        "id": "Oqetc7uLzsFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF\n",
        "\n",
        "TF-IDF technique is used to find meaning of sentences consisting of words and cancels out the incapabilities of Bag of Words technique which is good for text classification or for helping a machine read words in numbers.\n",
        "\n",
        "The number of times a term occurs in a document is called its Term frequency (TF).\n",
        "\n",
        "Document frequency is the number of documents in which the word is present. Inverse DF (IDF) is the inverse of the document frequency which measures the informativeness of term t."
      ],
      "metadata": {
        "id": "nf_ZD1BqzwYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createTFIDF(train, test, remove_stopwords, lemmatize, stemmer):\n",
        "    if remove_stopwords:\n",
        "        vectorizer = TfidfVectorizer(analyzer='word', input='content', stop_words=stopwords.words('english'))\n",
        "    else:\n",
        "        vectorizer =  TfidfVectorizer(analyzer='word', input='content')\n",
        "\n",
        "    clean_train = []\n",
        "    for paragraph in train:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_train.append(paragraph)\n",
        "\n",
        "    clean_test = []\n",
        "    for paragraph in test:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_test.append(paragraph)\n",
        "\n",
        "    tfidf_train = vectorizer.fit_transform(clean_train).toarray()\n",
        "    tfidf_test = vectorizer.transform(clean_test).toarray()\n",
        "    return tfidf_train, tfidf_test"
      ],
      "metadata": {
        "id": "VpkZGY6Mz1Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNDERSTANDING THE DATA : A REVIEWS DATASET\n",
        "\n",
        "Sentiment analysis is the interpretation and classification of emotions (such as positive, negative and neutral) within text data using text analysis techniques.\n",
        "\n",
        "Given below is a dataset consisting of reviews along with sentiment class (positive or negative)."
      ],
      "metadata": {
        "id": "N8qqL2mez5Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the Reviews CSV file that has been shared with you.\n",
        "# Run this cell, click on the 'Choose files' button and upload the file.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "FX2aJSUS0FLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('reviews.csv',sep='\\t')\n",
        "df.head(20)"
      ],
      "metadata": {
        "id": "wwfQ-ImL0IYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN MODEL\n",
        "\n",
        "Given below are two KNN models; in the first case we are using Bag-of-Words and in the second case we are using TF-IDF. Note the different metrics and parameters used in each."
      ],
      "metadata": {
        "id": "jYq2RK6n0RZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics, neighbors\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "\n",
        "## TASK - 1: Tweak the models below and see results with different parameters and distance metrics.\n",
        "\n",
        "def bow_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using bag-of-words and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('reviews.csv',sep='\\t')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"sentence\"], training_data[\"sentiment\"], test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createBagOfWords(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "    # print(X_train)\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='euclidean', metric_params=None, n_jobs=1)\n",
        "\n",
        "    knn.fit(X_train, y_train)\n",
        "    predicted = knn.predict(X_test)\n",
        "    acc = metrics.accuracy_score(y_test, predicted)\n",
        "    print('KNN with BOW accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    print(scores)\n",
        "    print('\\n')\n",
        "    return predicted, y_test\n",
        "\n",
        "\n",
        "def tfidf_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using tf-idf and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('reviews.csv',sep='\\t')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"sentence\"], training_data[\"sentiment\"],\n",
        "                                                        test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createTFIDF(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "    # print(X_train)\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='brute', leaf_size=30, p=2,\n",
        "                                         metric='cosine', metric_params=None, n_jobs=1)\n",
        "\n",
        "    knn.fit(X_train, y_train)\n",
        "    predicted = knn.predict(X_test)\n",
        "    acc = metrics.accuracy_score(y_test, predicted)\n",
        "    print('KNN with TFIDF accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    print(scores)\n",
        "    return predicted, y_test"
      ],
      "metadata": {
        "id": "SSBqwIIA0bZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Cross-validation will be discussed in detail in the upcoming lab session."
      ],
      "metadata": {
        "id": "3LReA2Ts0grA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## KNN accuracy after using BoW\n",
        "predicted, y_test = bow_knn()"
      ],
      "metadata": {
        "id": "UiSZm_vl0jKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## KNN accuracy after using TFIDF\n",
        "predicted, y_test = tfidf_knn()"
      ],
      "metadata": {
        "id": "JB-kescN0l_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1**"
      ],
      "metadata": {
        "id": "VjtcFtK_0rLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->For manhattan distance and cosine metric distance the accuracy of bag of words and also it's cross validation accuracy is more than the Euclidean distance metric.\n",
        "\n",
        "-->For euclidean metric distance and manhattan distance TFIDF accuracy is decreased it is better with cosine metric only.\n",
        "\n",
        "-->By tweaking the neighbours accuracy is changing accordingly."
      ],
      "metadata": {
        "id": "uA4xMfSQ0xxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPAM TEXT DATASET\n",
        "\n",
        "Now let's use what we've learnt to classify texts as spam or not spam."
      ],
      "metadata": {
        "id": "TGZnOkfX00mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the spam text data CSV file that has been shared with you.\n",
        "# Run this cell, click on the 'Choose files' button and upload the file.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "1vMrnQvp0_VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('spam_text_data.csv', error_bad_lines=False)\n",
        "df.head(20)"
      ],
      "metadata": {
        "id": "bNjM2y171B7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})"
      ],
      "metadata": {
        "id": "rjWLlBSL1Fse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "7lXWbADw1H91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "1ypLL7Qd1KbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics, neighbors\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "\n",
        "## TASK - 2: Tweak the models below and see results with different parameters and distance metrics.\n",
        "\n",
        "def bow_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using bag-of-words and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('spam_text_data.csv')\n",
        "    training_data['Category'] = training_data['Category'].map({'ham': 0, 'spam': 1})\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"Message\"], training_data[\"Category\"], test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createBagOfWords(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='euclidean', metric_params=None, n_jobs=1)\n",
        "\n",
        "    knn.fit(X_train, y_train)\n",
        "    predicted = knn.predict(X_test)\n",
        "    acc = metrics.accuracy_score(y_test, predicted)\n",
        "    print('KNN with BOW accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    print(scores)\n",
        "    print('\\n')\n",
        "    return predicted, y_test\n",
        "\n",
        "\n",
        "def tfidf_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using tf-idf and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('spam_text_data.csv')\n",
        "    training_data['Category'] = training_data['Category'].map({'ham': 0, 'spam': 1})\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"Message\"], training_data[\"Category\"], test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createTFIDF(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='brute', leaf_size=30, p=2, metric='manhattan', metric_params=None, n_jobs=1)\n",
        "\n",
        "    knn.fit(X_train, y_train)\n",
        "    predicted = knn.predict(X_test)\n",
        "    acc = metrics.accuracy_score(y_test, predicted)\n",
        "    print('KNN with TFIDF accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    print(scores)\n",
        "    return predicted, y_test"
      ],
      "metadata": {
        "id": "pnPqfAK71NeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell may take some time to run\n",
        "predicted, y_test = bow_knn()"
      ],
      "metadata": {
        "id": "50iE0msS1QgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell may take some time to run\n",
        "predicted, y_test = tfidf_knn()"
      ],
      "metadata": {
        "id": "Tm8TxBRX1Sp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2**"
      ],
      "metadata": {
        "id": "qZQfY5431W8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.For Cosine distance metric accuracy of Bow is more than the euclidean and manhatten distances.\n",
        "\n",
        "2.The increase in number of neighbours in Bow knn accuracy is decreasing.\n",
        "\n",
        "3.The accuracy of TFIDF is better with cosin metric and for remaining distances it is low accurate."
      ],
      "metadata": {
        "id": "Kbj9VCHX1fFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions to Think About and Answer**\n",
        "\n",
        "1.Why does the TF-IDF approach generally result in a better accuracy than Bag-of-Words ?\n",
        "\n",
        "2.Can you think of techniques that are better than both BoW and TF-IDF ?\n",
        "\n",
        "3.Read about Stemming and Lemmatization from the resources given below. Think about the pros/cons of each."
      ],
      "metadata": {
        "id": "DJqG6vev1oAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.Why does the TF-IDF approach generally result in a better accuracy than Bag-of-Words ?"
      ],
      "metadata": {
        "id": "kvWwfFbO1yK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the TF-IDF model contains information on the more important words and the less important ones as well but Bag of Words just creates a set of vectors containing the count of word occurrences in the document.So TFIDF contains more information according to the words in document than the BOW that's the reason for better accuracy of TFIDF than BAG-OF-WORDS."
      ],
      "metadata": {
        "id": "8sC95IZ612WV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Can you think of techniques that are better than both BoW and TF-IDF ?"
      ],
      "metadata": {
        "id": "o69HPdLx15vc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can think of word2vec,CBOW and Skip-gram model but these are not better than BoW and TF-IDF."
      ],
      "metadata": {
        "id": "bfAAoy-o1-yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Read about Stemming and Lemmatization from the resources given below. Think about the pros/cons of each."
      ],
      "metadata": {
        "id": "gIdn1cDe2BsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form\n",
        "\n",
        "1.Lemmatization is slower as compared to stemming but it knows the context of the word before proceeding.\n",
        "\n",
        "2.Stemming is preferred when the meaning of the word is not important for analysis.\n",
        "\n",
        "3.Lemmatization is more accurate than the stemming.\n",
        "\n",
        "4.Stemming causes meaningless to the words inthe document.\n",
        "\n",
        "5.Both these techniques are better for data normalization and each are good in their own way.\n"
      ],
      "metadata": {
        "id": "ZJ6CB3nD2HWN"
      }
    }
  ]
}